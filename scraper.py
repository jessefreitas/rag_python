import asyncio
import logging
import re
import random
import time
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup, Comment

# Importa√ß√µes para integra√ß√£o OpenAI
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from openai import OpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """
    Limpa e normaliza o texto extra√≠do, removendo elementos desnecess√°rios.
    """
    if not text:
        return ""
    
    # Remover padr√µes CSS comuns
    css_patterns = [
        r'--[a-zA-Z-]+:\s*[^;]+;',  # Vari√°veis CSS
        r'[a-zA-Z-]+:\s*[^;]+;',    # Propriedades CSS
        r'@[a-zA-Z-]+[^{]*{[^}]*}', # At-rules CSS
        r'\.[\w-]+\s*{[^}]*}',      # Classes CSS
        r'#[\w-]+\s*{[^}]*}',       # IDs CSS
        r'[\w-]+\s*:\s*[\w\s\(\),-]+;', # Propriedades CSS gen√©ricas
        r'rgb\([^)]+\)',            # Cores RGB
        r'rgba\([^)]+\)',           # Cores RGBA
        r'#[0-9a-fA-F]{3,8}',       # Cores hexadecimais
        r'px|em|rem|%|vh|vw|pt',    # Unidades CSS
        r'font-family|font-size|font-weight|color|background|margin|padding|border|width|height', # Propriedades comuns
    ]
    
    for pattern in css_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # Remover m√∫ltiplas quebras de linha e espa√ßos
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\s*{\s*', ' ', text)
    text = re.sub(r'\s*}\s*', ' ', text)
    
    # Remover linhas que s√£o apenas pontua√ß√£o ou caracteres especiais
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        # Filtrar linhas que parecem ser CSS ou c√≥digo
        if (line and 
            len(line) > 3 and 
            not re.match(r'^[\{\}\[\]\(\):;,\-\s]*$', line) and
            not re.match(r'^[a-zA-Z-]+:\s*.*$', line) and  # Propriedades CSS
            not re.match(r'^--[a-zA-Z-]+:', line) and      # Vari√°veis CSS
            not re.match(r'^\.[a-zA-Z-]', line) and        # Classes CSS
            not re.match(r'^#[a-zA-Z-]', line) and         # IDs CSS
            not ':' in line[:20] or ' ' in line[:20]):     # Evitar propriedades CSS
            cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def extract_main_content_from_html(html_content: str) -> dict:
    """
    Extrai m√∫ltiplas vers√µes do conte√∫do do HTML para posterior processamento.
    Retorna um dicion√°rio com diferentes extra√ß√µes.
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Remover elementos que nunca cont√™m conte√∫do √∫til
    unwanted_elements = [
        'script', 'style', 'noscript', 'iframe', 'embed', 'object', 'svg',
        'meta', 'link', 'title', 'base', 'area', 'map'
    ]
    
    for selector in unwanted_elements:
        for element in soup.select(selector):
            element.decompose()
    
    # Remover coment√°rios HTML
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()
    
    # Extrair diferentes tipos de conte√∫do
    content_extractions = {
        'title': soup.title.get_text(strip=True) if soup.title else "",
        'meta_description': "",
        'headings': [],
        'paragraphs': [],
        'lists': [],
        'semantic_content': "",
        'full_body_text': "",
        'jurisprudence_content': ""  # Espec√≠fico para sites jur√≠dicos
    }
    
    # Meta description
    meta_desc = soup.find('meta', attrs={'name': 'description'})
    if meta_desc:
        content_extractions['meta_description'] = meta_desc.get('content', '')
    
    # Extrair headings (h1-h6)
    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
        text = heading.get_text(strip=True)
        if text and len(text) > 2:
            content_extractions['headings'].append({
                'level': heading.name,
                'text': text
            })
    
    # Extrair par√°grafos
    for p in soup.find_all('p'):
        text = p.get_text(strip=True)
        if text and len(text) > 10:
            content_extractions['paragraphs'].append(text)
    
    # Extrair listas
    for ul in soup.find_all(['ul', 'ol']):
        items = []
        for li in ul.find_all('li'):
            text = li.get_text(strip=True)
            if text and len(text) > 3:
                items.append(text)
        if items:
            content_extractions['lists'].append(items)
    
    # Extrair conte√∫do sem√¢ntico (main, article, section)
    semantic_selectors = [
        'main', 'article', 'section',
        '[role="main"]', '.content', '.main-content',
        '.article-content', '.post-content', '.entry-content'
    ]
    
    semantic_texts = []
    for selector in semantic_selectors:
        elements = soup.select(selector)
        for element in elements:
            text = element.get_text(separator='\n', strip=True)
            if text and len(text) > 50:
                semantic_texts.append(text)
    
    content_extractions['semantic_content'] = '\n\n'.join(semantic_texts)
    
    # Extrair conte√∫do jur√≠dico espec√≠fico
    jurisprudence_selectors = [
        '.jurisprudencia-content', '.decisao-content', '.ementa',
        '.acordao', '.sentenca', '.voto', '.relatorio', '.fundamentacao',
        '.tribunal', '.processo', '.julgamento', '.decisao'
    ]
    
    jurisprudence_texts = []
    for selector in jurisprudence_selectors:
        elements = soup.select(selector)
        for element in elements:
            text = element.get_text(separator='\n', strip=True)
            if text and len(text) > 20:
                jurisprudence_texts.append(text)
    
    content_extractions['jurisprudence_content'] = '\n\n'.join(jurisprudence_texts)
    
    # Extrair texto completo do body como fallback
    if soup.body:
        content_extractions['full_body_text'] = soup.body.get_text(separator='\n', strip=True)
    
    return content_extractions

def process_content_extractions(extractions: dict, url: str, protected: bool = False) -> str:
    """
    Processa as extra√ß√µes de conte√∫do e retorna o texto limpo final.
    Vers√£o melhorada para sites premium.
    """
    # Detectar se √© site premium
    is_premium = any(domain in url.lower() for domain in [
        'thomsonreuters', 'westlaw', 'lexisnexis', 'vlex', 'proview'
    ])
    
    # Para sites premium, tentar estrat√©gias mais agressivas primeiro
    if is_premium:
        logger.info("üè¢ Processando site premium")
        
        # Tentar usar qualquer conte√∫do dispon√≠vel, mesmo que pequeno
        content_candidates = [
            ('jurisprudence_content', "‚öñÔ∏è Conte√∫do jur√≠dico premium"),
            ('semantic_content', "üìÑ Conte√∫do sem√¢ntico premium"), 
            ('full_body_text', "üåê Texto completo premium"),
            ('paragraphs', "üìù Par√°grafos premium")
        ]
        
        for key, description in content_candidates:
            content = extractions.get(key, '')
            if isinstance(content, list):
                content = '\n\n'.join(content)
            
            if content and len(content.strip()) > 50:  # Limiar mais baixo para premium
                logger.info(description)
                main_content = content
                break
        else:
            # Se n√£o encontrou conte√∫do, tentar headings
            if extractions.get('headings'):
                headings_text = '\n'.join([h.get('text', '') if isinstance(h, dict) else str(h) 
                                         for h in extractions['headings']])
                if len(headings_text.strip()) > 20:
                    logger.info("üìã Usando headings de site premium")
                    main_content = headings_text
                else:
                    main_content = None
            else:
                main_content = None
    else:
        # L√≥gica normal para sites n√£o-premium
        # Priorizar conte√∫do jur√≠dico se dispon√≠vel
        if extractions['jurisprudence_content'] and len(extractions['jurisprudence_content'].strip()) > 100:
            main_content = extractions['jurisprudence_content']
            logger.info("üèõÔ∏è Usando conte√∫do jur√≠dico espec√≠fico")
        
        # Sen√£o, usar conte√∫do sem√¢ntico
        elif extractions['semantic_content'] and len(extractions['semantic_content'].strip()) > 200:
            main_content = extractions['semantic_content']
            logger.info("üìÑ Usando conte√∫do sem√¢ntico")
        
        # Combinar par√°grafos se necess√°rio
        elif extractions['paragraphs'] and len(extractions['paragraphs']) > 2:
            main_content = '\n\n'.join(extractions['paragraphs'])
            logger.info("üìù Usando par√°grafos combinados")
        
        # Usar texto completo do body como √∫ltimo recurso
        elif extractions['full_body_text'] and len(extractions['full_body_text'].strip()) > 100:
            main_content = extractions['full_body_text']
            logger.info("üåê Usando texto completo do body")
        
        else:
            main_content = None
    
    # Se n√£o h√° conte√∫do suficiente
    if not main_content:
        basic_info = f"T√≠tulo: {extractions['title']}\nURL: {url}\n\n"
        
        if is_premium:
            basic_info += "‚ö†Ô∏è Site premium detectado - Conte√∫do pode estar protegido por autentica√ß√£o.\n"
            basic_info += "Poss√≠veis causas:\n"
            basic_info += "- Conte√∫do requer login/assinatura\n"
            basic_info += "- JavaScript complexo n√£o executado completamente\n"
            basic_info += "- Prote√ß√£o anti-scraping avan√ßada\n"
            basic_info += "- Conte√∫do carregado dinamicamente via AJAX\n\n"
            basic_info += "Sugest√£o: Verificar se o acesso direto √† URL funciona no navegador.\n\n"
        elif protected:
            basic_info += "‚ö†Ô∏è Esta p√°gina est√° protegida e n√£o permitiu extra√ß√£o autom√°tica de conte√∫do.\n"
            basic_info += "Para acessar o conte√∫do, visite a URL diretamente no navegador.\n\n"
        else:
            basic_info += "‚ö†Ô∏è Conte√∫do textual n√£o p√¥de ser extra√≠do desta p√°gina.\n"
            basic_info += "Poss√≠veis causas: conte√∫do gerado por JavaScript, prote√ß√£o anti-bot, ou estrutura complexa.\n\n"
        
        # Adicionar headings se dispon√≠veis
        if extractions['headings']:
            basic_info += "T√≠tulos encontrados:\n"
            for heading in extractions['headings'][:5]:  # M√°ximo 5 t√≠tulos
                heading_text = heading.get('text', '') if isinstance(heading, dict) else str(heading)
                if heading_text.strip():
                    basic_info += f"- {heading_text}\n"
        
        return basic_info
    
    # Limpar o conte√∫do selecionado
    cleaned_content = clean_text(main_content)
    
    # Filtros adicionais para sites protegidos
    if cleaned_content:
        blocked_phrases = [
            'verifying you are human', 'just a moment', 'please wait',
            'checking your browser', 'ddos protection', 'cloudflare',
            'ray id', 'performance & security by', 'enable javascript',
            'browser does not support', 'cookies must be enabled'
        ]
        
        lines = cleaned_content.split('\n')
        final_lines = []
        
        for line in lines:
            line = line.strip()
            line_lower = line.lower()
            
            # Verificar se a linha n√£o cont√©m frases de bloqueio
            is_blocked = any(phrase in line_lower for phrase in blocked_phrases)
            
            if (line and 
                len(line) > 5 and
                not is_blocked and
                not any(css_word in line_lower for css_word in [
                    'css', 'style', 'font-family', 'color:', 'background',
                    'margin:', 'padding:', 'border:', 'width:', 'height:',
                    'rgba(', 'rgb(', '--', 'px', 'em', 'rem', '%'
                ]) and
                not re.match(r'^[{}:;,\s]*$', line)):
                final_lines.append(line)
        
        cleaned_content = '\n'.join(final_lines)
    
    return cleaned_content

async def human_like_behavior(page):
    """
    Simula comportamento humano para evitar detec√ß√£o por sistemas anti-bot.
    """
    # Movimento aleat√≥rio do mouse
    await page.mouse.move(
        random.randint(100, 800), 
        random.randint(100, 600)
    )
    
    # Scroll aleat√≥rio
    await page.evaluate(f"window.scrollTo(0, {random.randint(100, 500)})")
    await page.wait_for_timeout(random.randint(500, 1500))
    
    # Mais um scroll
    await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
    await page.wait_for_timeout(random.randint(1000, 2000))

async def detect_cloudflare_protection(page):
    """
    Detecta se a p√°gina tem prote√ß√£o Cloudflare ativa.
    """
    try:
        # Verificar indicadores comuns de Cloudflare
        cloudflare_indicators = [
            'Verifying you are human',
            'Just a moment',
            'Please wait',
            'Checking your browser',
            'DDoS protection',
            'Ray ID',
            'cf-ray',
            'cloudflare'
        ]
        
        page_content = await page.content()
        page_text = await page.inner_text('body') if await page.query_selector('body') else ""
        
        for indicator in cloudflare_indicators:
            if indicator.lower() in page_content.lower() or indicator.lower() in page_text.lower():
                return True
                
        # Verificar elementos espec√≠ficos do Cloudflare
        cf_selectors = [
            '[data-ray]',
            '.cf-error-details',
            '.cf-wrapper',
            '#cf-content',
            '.cf-browser-verification'
        ]
        
        for selector in cf_selectors:
            if await page.query_selector(selector):
                return True
                
        return False
        
    except Exception:
        return False

async def scrape_url_to_json(url: str) -> dict:
    """
    Captura uma p√°gina e retorna todos os dados em formato JSON para posterior processamento.
    Vers√£o melhorada para sites com muito JavaScript.
    """
    logger.info(f"üîç Iniciando captura JSON da URL: {url}")
    
    try:
        async with async_playwright() as p:
            # Configurar browser com flags anti-detec√ß√£o
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-automation',
                    '--disable-extensions',
                    '--no-first-run',
                    '--disable-default-apps',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding',
                    '--disable-field-trial-config',
                    '--disable-back-forward-cache',
                    '--disable-ipc-flooding-protection',
                    '--enable-javascript',  # Garantir que JS est√° habilitado
                    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
                ]
            )
            
            # Criar contexto com configura√ß√µes realistas
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                locale='pt-BR',
                timezone_id='America/Sao_Paulo',
                permissions=['geolocation'],
                java_script_enabled=True,  # Explicitamente habilitar JavaScript
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1',
                    'Cache-Control': 'max-age=0'
                }
            )
            
            page = await context.new_page()
            
            # Mascarar propriedades que indicam automa√ß√£o
            await page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5],
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['pt-BR', 'pt', 'en'],
                });
                
                window.chrome = {
                    runtime: {},
                };
                
                Object.defineProperty(navigator, 'permissions', {
                    get: () => ({
                        query: () => Promise.resolve({ state: 'granted' }),
                    }),
                });
            """)
            
            # Para sites premium, permitir mais recursos
            is_premium_site = any(domain in url.lower() for domain in [
                'thomsonreuters', 'westlaw', 'lexisnexis', 'vlex', 'proview'
            ])
            
            if is_premium_site:
                # Para sites premium, permitir CSS e outros recursos
                await page.route("**/*", lambda route: (
                    route.abort() if route.request.resource_type in [
                        "image", "font", "media"
                    ] else route.continue_()
                ))
                logger.info("üè¢ Site premium detectado - permitindo mais recursos")
            else:
                # Para sites normais, bloquear mais recursos
                await page.route("**/*", lambda route: (
                    route.abort() if route.request.resource_type in [
                        "image", "font", "media"
                    ] else route.continue_()
                ))
            
            # Delay inicial aleat√≥rio
            await page.wait_for_timeout(random.randint(2000, 4000))
            
            # Navegar para a p√°gina
            logger.info(f"üåê Navegando para: {url}")
            await page.goto(url, timeout=45000, wait_until='networkidle')  # Aguardar rede idle
            
            # Aguardar carregamento inicial mais longo para sites JS-heavy
            await page.wait_for_timeout(random.randint(5000, 8000))
            
            # Verificar se h√° prote√ß√£o Cloudflare
            has_cloudflare = await detect_cloudflare_protection(page)
            
            if has_cloudflare:
                logger.warning(f"üõ°Ô∏è Prote√ß√£o Cloudflare detectada em {url}")
                
                # Tentar aguardar mais tempo para verifica√ß√£o autom√°tica
                logger.info("‚è≥ Aguardando verifica√ß√£o autom√°tica...")
                await page.wait_for_timeout(random.randint(10000, 15000))
                
                # Simular comportamento humano
                await human_like_behavior(page)
                
                # Aguardar mais um pouco
                await page.wait_for_timeout(random.randint(5000, 10000))
                
                # Verificar novamente se ainda h√° prote√ß√£o
                has_cloudflare = await detect_cloudflare_protection(page)
                
                if has_cloudflare:
                    logger.warning(f"üö´ N√£o foi poss√≠vel contornar a prote√ß√£o de {url}")
            
            # Para sites premium, tentar executar JavaScript adicional
            if is_premium_site:
                logger.info("üîß Executando JavaScript adicional para site premium...")
                
                # Tentar for√ßar carregamento de conte√∫do
                try:
                    await page.evaluate("""
                        // Scroll para ativar lazy loading
                        window.scrollTo(0, document.body.scrollHeight / 2);
                        
                        // Aguardar um pouco
                        await new Promise(resolve => setTimeout(resolve, 2000));
                        
                        // Tentar clicar em elementos que podem carregar conte√∫do
                        const buttons = document.querySelectorAll('button, [role="button"], .load-more, .show-more');
                        for (let btn of buttons) {
                            if (btn.textContent && btn.textContent.toLowerCase().includes('load')) {
                                btn.click();
                                break;
                            }
                        }
                        
                        // Aguardar mais um pouco
                        await new Promise(resolve => setTimeout(resolve, 3000));
                    """)
                    await page.wait_for_timeout(3000)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao executar JavaScript adicional: {e}")
            
            # Simular mais comportamento humano
            await human_like_behavior(page)
            
            # Aguardar conte√∫do din√¢mico - mais tempo para sites premium
            wait_time = random.randint(8000, 12000) if is_premium_site else random.randint(3000, 6000)
            await page.wait_for_timeout(wait_time)
            
            # Tentar aguardar por seletores espec√≠ficos de conte√∫do
            content_selectors = [
                'main', 'article', '[role="main"]', '.content', '.main-content',
                '.document-content', '.legal-content', '.case-content', '.text-content'
            ]
            
            for selector in content_selectors:
                try:
                    await page.wait_for_selector(selector, timeout=5000)
                    logger.info(f"‚úÖ Conte√∫do encontrado: {selector}")
                    break
                except:
                    continue
            
            # Extrair todos os dados
            title = await page.title()
            html_content = await page.content()
            current_url = page.url
            
            await browser.close()
            
            # Criar objeto JSON com todos os dados
            page_data = {
                'capture_info': {
                    'original_url': url,
                    'final_url': current_url,
                    'timestamp': datetime.now().isoformat(),
                    'protected': has_cloudflare,
                    'is_premium_site': is_premium_site,
                    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
                },
                'page_metadata': {
                    'title': title,
                    'html_length': len(html_content)
                },
                'raw_html': html_content,
                'content_extractions': extract_main_content_from_html(html_content)
            }
            
            logger.info(f"üì¶ Dados JSON capturados - T√≠tulo: '{title}' | HTML: {len(html_content)} chars")
            
            return {
                "success": True,
                "data": page_data
            }
            
    except Exception as e:
        error_msg = f"Erro ao capturar JSON da URL {url}: {str(e)}"
        logger.error(f"‚ùå {error_msg}", exc_info=True)
        return {
            "success": False,
            "error": error_msg,
            "url": url
        }

def process_json_to_clean_text(json_data: dict) -> dict:
    """
    Processa os dados JSON e retorna apenas o texto limpo.
    """
    if not json_data.get('success'):
        return json_data
    
    try:
        page_data = json_data['data']
        capture_info = page_data['capture_info']
        extractions = page_data['content_extractions']
        
        # Processar extra√ß√µes para texto limpo
        clean_content = process_content_extractions(
            extractions, 
            capture_info['original_url'], 
            capture_info['protected']
        )
        
        # Validar conte√∫do final
        if not clean_content.strip() or len(clean_content.strip()) < 30:
            clean_content = f"P√°gina: {page_data['page_metadata']['title']}\n"
            clean_content += f"URL: {capture_info['original_url']}\n\n"
            clean_content += "‚ö†Ô∏è Conte√∫do textual limitado extra√≠do desta p√°gina.\n"
        
        result = {
            "success": True,
            "title": page_data['page_metadata']['title'],
            "content": clean_content,
            "url": capture_info['original_url'],
            "content_length": len(clean_content),
            "protected": capture_info['protected'],
            "capture_timestamp": capture_info['timestamp']
        }
        
        logger.info(f"‚úÖ Processamento JSON conclu√≠do - Conte√∫do: {len(clean_content)} caracteres")
        return result
        
    except Exception as e:
        error_msg = f"Erro ao processar JSON: {str(e)}"
        logger.error(f"‚ùå {error_msg}", exc_info=True)
        return {
            "success": False,
            "error": error_msg
        }

async def scrape_url_content(url: str) -> dict:
    """
    Fun√ß√£o principal que captura para JSON e depois processa para texto limpo.
    """
    # Primeiro: capturar tudo em JSON
    json_result = await scrape_url_to_json(url)
    
    if not json_result['success']:
        return json_result
    
    # Segundo: processar JSON para texto limpo
    final_result = process_json_to_clean_text(json_result)
    
    return final_result

def scrape_url(url: str) -> dict:
    """Fun√ß√£o s√≠ncrona que envolve a fun√ß√£o ass√≠ncrona scrape_url_content."""
    return asyncio.run(scrape_url_content(url))

def save_page_as_json(url: str, filename: str = None) -> dict:
    """
    Salva uma p√°gina como JSON para an√°lise posterior.
    """
    json_result = asyncio.run(scrape_url_to_json(url))
    
    if json_result['success'] and filename:
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(json_result['data'], f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ JSON salvo em: {filename}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar JSON: {e}")
    
    return json_result

def process_saved_json(filename: str) -> dict:
    """
    Processa um arquivo JSON salvo e extrai o texto limpo.
    """
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            page_data = json.load(f)
        
        json_result = {"success": True, "data": page_data}
        return process_json_to_clean_text(json_result)
        
    except Exception as e:
        error_msg = f"Erro ao processar arquivo JSON {filename}: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        return {
            "success": False,
            "error": error_msg
        }

def test_scraper():
    """Fun√ß√£o para testar o scraper com diferentes tipos de sites."""
    test_urls = [
        'https://www.example.com',
        'https://www.jusbrasil.com.br/jurisprudencia/trt-4/922418931',
        'https://pt.wikipedia.org/wiki/Intelig%C3%AAncia_artificial'
    ]
    
    for url in test_urls:
        print(f"\n{'='*70}")
        print(f"üß™ Testando SCRAPER JSON->TEXTO: {url}")
        print('='*70)
        
        result = scrape_url(url)
        
        if result['success']:
            print(f"‚úÖ Sucesso!")
            print(f"üìÑ T√≠tulo: {result['title']}")
            print(f"üìè Tamanho do conte√∫do: {result['content_length']} caracteres")
            if result.get('protected'):
                print(f"üõ°Ô∏è P√°gina protegida detectada")
            print(f"üìù Pr√©via do conte√∫do (400 chars):")
            print("-" * 50)
            print(f"{result['content'][:400]}...")
            print("-" * 50)
        else:
            print(f"‚ùå Erro: {result['error']}")

def chunk_text_with_openai(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:
    """
    Divide o texto em chunks usando RecursiveCharacterTextSplitter e prepara para embeddings.
    
    Args:
        text: Texto a ser dividido
        chunk_size: Tamanho m√°ximo de cada chunk
        chunk_overlap: Sobreposi√ß√£o entre chunks
        
    Returns:
        Lista de documentos LangChain prontos para embeddings
    """
    try:
        # Configurar o text splitter
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=[
                "\n\n",  # Par√°grafos
                "\n",    # Linhas
                ".",     # Senten√ßas
                "!",     # Exclama√ß√µes
                "?",     # Perguntas
                ";",     # Ponto e v√≠rgula
                ",",     # V√≠rgulas
                " ",     # Espa√ßos
                ""       # Caracteres individuais
            ]
        )
        
        # Dividir o texto em chunks
        chunks = text_splitter.split_text(text)
        
        # Converter em documentos LangChain
        documents = []
        for i, chunk in enumerate(chunks):
            if chunk.strip():  # Apenas chunks n√£o vazios
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "chunk_index": i,
                        "chunk_size": len(chunk),
                        "total_chunks": len(chunks),
                        "source": "web_scraper",
                        "timestamp": datetime.now().isoformat()
                    }
                )
                documents.append(doc)
        
        logger.info(f"üìÑ Texto dividido em {len(documents)} chunks (tamanho: {chunk_size}, overlap: {chunk_overlap})")
        return documents
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao dividir texto em chunks: {e}")
        return []

def generate_embeddings_with_openai(texts: List[str], model: str = "text-embedding-3-small") -> List[List[float]]:
    """
    Gera embeddings usando a API da OpenAI.
    
    Args:
        texts: Lista de textos para gerar embeddings
        model: Modelo de embedding da OpenAI
        
    Returns:
        Lista de embeddings (vetores)
    """
    try:
        # Verificar se a API key est√° configurada
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("‚ùå OPENAI_API_KEY n√£o configurada")
            return []
        
        # Inicializar cliente OpenAI
        client = OpenAI(api_key=api_key)
        
        # Filtrar textos vazios
        valid_texts = [text.strip() for text in texts if text.strip()]
        if not valid_texts:
            logger.warning("‚ö†Ô∏è Nenhum texto v√°lido para gerar embeddings")
            return []
        
        logger.info(f"üîÆ Gerando embeddings para {len(valid_texts)} textos usando modelo {model}")
        
        # Gerar embeddings
        response = client.embeddings.create(
            input=valid_texts,
            model=model
        )
        
        # Extrair os vetores
        embeddings = [embedding.embedding for embedding in response.data]
        
        logger.info(f"‚úÖ {len(embeddings)} embeddings gerados com sucesso")
        return embeddings
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao gerar embeddings: {e}")
        return []

def process_scraped_content_for_rag(scraped_result: dict, 
                                   chunk_size: int = 1000, 
                                   chunk_overlap: int = 200,
                                   generate_embeddings: bool = True) -> dict:
    """
    Processa o conte√∫do extra√≠do pelo scraper para uso no sistema RAG.
    
    Args:
        scraped_result: Resultado do scraper
        chunk_size: Tamanho dos chunks
        chunk_overlap: Sobreposi√ß√£o entre chunks
        generate_embeddings: Se deve gerar embeddings
        
    Returns:
        Dicion√°rio com chunks e embeddings processados
    """
    try:
        if not scraped_result.get('success'):
            return {
                "success": False,
                "error": scraped_result.get('error', 'Scraping falhou'),
                "chunks": [],
                "embeddings": []
            }
        
        content = scraped_result.get('content', '')
        if not content.strip():
            return {
                "success": False,
                "error": "Conte√∫do vazio",
                "chunks": [],
                "embeddings": []
            }
        
        logger.info(f"üîÑ Processando conte√∫do para RAG: {len(content)} caracteres")
        
        # 1. Dividir em chunks
        documents = chunk_text_with_openai(content, chunk_size, chunk_overlap)
        
        if not documents:
            return {
                "success": False,
                "error": "Falha ao dividir texto em chunks",
                "chunks": [],
                "embeddings": []
            }
        
        # 2. Preparar dados dos chunks
        chunks_data = []
        chunk_texts = []
        
        for doc in documents:
            chunk_info = {
                "text": doc.page_content,
                "metadata": doc.metadata,
                "length": len(doc.page_content)
            }
            chunks_data.append(chunk_info)
            chunk_texts.append(doc.page_content)
        
        # 3. Gerar embeddings se solicitado
        embeddings = []
        if generate_embeddings:
            logger.info("üîÆ Gerando embeddings para os chunks...")
            embeddings = generate_embeddings_with_openai(chunk_texts)
            
            if len(embeddings) != len(chunk_texts):
                logger.warning(f"‚ö†Ô∏è N√∫mero de embeddings ({len(embeddings)}) diferente do n√∫mero de chunks ({len(chunk_texts)})")
        
        # 4. Combinar chunks com embeddings
        for i, chunk in enumerate(chunks_data):
            if i < len(embeddings):
                chunk["embedding"] = embeddings[i]
                chunk["embedding_dimensions"] = len(embeddings[i])
            else:
                chunk["embedding"] = None
                chunk["embedding_dimensions"] = 0
        
        result = {
            "success": True,
            "source_url": scraped_result.get('url', ''),
            "source_title": scraped_result.get('title', ''),
            "original_content_length": len(content),
            "total_chunks": len(chunks_data),
            "chunks": chunks_data,
            "embeddings": embeddings,
            "processing_info": {
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "embedding_model": "text-embedding-3-small",
                "timestamp": datetime.now().isoformat()
            }
        }
        
        logger.info(f"‚úÖ Processamento RAG conclu√≠do: {len(chunks_data)} chunks, {len(embeddings)} embeddings")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao processar conte√∫do para RAG: {e}")
        return {
            "success": False,
            "error": str(e),
            "chunks": [],
            "embeddings": []
        }

def scrape_and_process_for_rag(url: str, 
                              chunk_size: int = 1000, 
                              chunk_overlap: int = 200) -> dict:
    """
    Fun√ß√£o completa que faz scraping de uma URL e processa para RAG.
    
    Args:
        url: URL para fazer scraping
        chunk_size: Tamanho dos chunks
        chunk_overlap: Sobreposi√ß√£o entre chunks
        
    Returns:
        Dicion√°rio com conte√∫do processado para RAG
    """
    logger.info(f"üöÄ Iniciando scraping e processamento RAG para: {url}")
    
    # 1. Fazer scraping
    scraped_result = scrape_url(url)
    
    # 2. Processar para RAG
    rag_result = process_scraped_content_for_rag(
        scraped_result, 
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
    
    # 3. Adicionar informa√ß√µes do scraping
    if rag_result.get('success'):
        rag_result['scraping_info'] = {
            "url": scraped_result.get('url', ''),
            "title": scraped_result.get('title', ''),
            "content_length": scraped_result.get('content_length', 0),
            "protected": scraped_result.get('protected', False),
            "capture_timestamp": scraped_result.get('capture_timestamp', '')
        }
    
    return rag_result

def save_rag_processed_content(rag_result: dict, filename: str = None) -> dict:
    """
    Salva o conte√∫do processado para RAG em arquivo JSON.
    
    Args:
        rag_result: Resultado do processamento RAG
        filename: Nome do arquivo (opcional)
        
    Returns:
        Resultado da opera√ß√£o de salvamento
    """
    try:
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            url_clean = re.sub(r'[^\w\-_\.]', '_', rag_result.get('source_url', 'unknown'))[:50]
            filename = f"rag_processed_{url_clean}_{timestamp}.json"
        
        # Salvar sem os embeddings para arquivo mais leve (embeddings s√£o grandes)
        save_data = rag_result.copy()
        if 'embeddings' in save_data:
            save_data['embeddings_count'] = len(save_data['embeddings'])
            del save_data['embeddings']  # Remover embeddings do arquivo
        
        # Remover embeddings dos chunks tamb√©m
        if 'chunks' in save_data:
            for chunk in save_data['chunks']:
                if 'embedding' in chunk:
                    chunk['has_embedding'] = chunk['embedding'] is not None
                    del chunk['embedding']
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ Conte√∫do RAG salvo em: {filename}")
        return {
            "success": True,
            "filename": filename,
            "chunks_saved": len(save_data.get('chunks', [])),
            "embeddings_generated": save_data.get('embeddings_count', 0)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao salvar conte√∫do RAG: {e}")
        return {
            "success": False,
            "error": str(e)
        }

def test_openai_integration():
    """Testa a integra√ß√£o com OpenAI para chunking e embeddings."""
    print("\nüß™ Testando Integra√ß√£o OpenAI - Chunking e Embeddings")
    print("=" * 60)
    
    # Testar com uma URL
    test_url = "https://www.example.com"
    print(f"\n1. Testando scraping + RAG processing: {test_url}")
    
    result = scrape_and_process_for_rag(test_url, chunk_size=500, chunk_overlap=100)
    
    if result['success']:
        print(f"‚úÖ Processamento conclu√≠do!")
        print(f"üìÑ URL: {result['scraping_info']['url']}")
        print(f"üìù T√≠tulo: {result['scraping_info']['title']}")
        print(f"üìè Conte√∫do original: {result['original_content_length']} caracteres")
        print(f"üß© Total de chunks: {result['total_chunks']}")
        print(f"üîÆ Embeddings gerados: {len(result['embeddings'])}")
        
        # Mostrar exemplo de chunk
        if result['chunks']:
            first_chunk = result['chunks'][0]
            print(f"\nüìã Exemplo do primeiro chunk:")
            print(f"   Tamanho: {first_chunk['length']} caracteres")
            print(f"   Texto: {first_chunk['text'][:200]}...")
            if first_chunk.get('embedding'):
                print(f"   Embedding: {len(first_chunk['embedding'])} dimens√µes")
        
        # Salvar resultado
        save_result = save_rag_processed_content(result)
        if save_result['success']:
            print(f"üíæ Salvo em: {save_result['filename']}")
    else:
        print(f"‚ùå Erro: {result['error']}")

def integrate_with_existing_rag_system(url: str, 
                                       agent_id: str,
                                       chunk_size: int = 1000, 
                                       chunk_overlap: int = 200) -> dict:
    """
    Integra o scraper com o sistema RAG existente do projeto.
    Faz scraping, chunking, embeddings e salva no banco PostgreSQL.
    
    Args:
        url: URL para fazer scraping
        agent_id: ID do agente para isolamento de seguran√ßa
        chunk_size: Tamanho dos chunks
        chunk_overlap: Sobreposi√ß√£o entre chunks
        
    Returns:
        Resultado da integra√ß√£o completa
    """
    try:
        logger.info(f"üîó Iniciando integra√ß√£o RAG completa para agente {agent_id}")
        
        # 1. Fazer scraping e processamento RAG
        rag_result = scrape_and_process_for_rag(url, chunk_size, chunk_overlap)
        
        if not rag_result['success']:
            return {
                "success": False,
                "error": f"Falha no scraping: {rag_result['error']}",
                "agent_id": agent_id,
                "url": url
            }
        
        # 2. Preparar documentos para o sistema existente
        try:
            from vector_store import PGVectorStore
            from langchain.schema import Document
            
            # Criar documentos LangChain com metadados completos
            documents = []
            for i, chunk in enumerate(rag_result['chunks']):
                doc = Document(
                    page_content=chunk['text'],
                    metadata={
                        "source": url,
                        "title": rag_result['source_title'],
                        "chunk_index": i,
                        "total_chunks": rag_result['total_chunks'],
                        "chunk_size": chunk['length'],
                        "url": url,
                        "agent_id": agent_id,
                        "scraping_timestamp": rag_result['scraping_info']['capture_timestamp'],
                        "processing_timestamp": rag_result['processing_info']['timestamp'],
                        "embedding_model": rag_result['processing_info']['embedding_model']
                    }
                )
                documents.append(doc)
            
            # 3. Salvar no sistema RAG existente
            vector_store = PGVectorStore(agent_id=agent_id)
            vector_store.add_documents(documents)
            
            logger.info(f"‚úÖ Integra√ß√£o RAG conclu√≠da: {len(documents)} chunks salvos no banco para agente {agent_id}")
            
            return {
                "success": True,
                "agent_id": agent_id,
                "url": url,
                "title": rag_result['source_title'],
                "chunks_saved": len(documents),
                "embeddings_generated": len(rag_result['embeddings']),
                "original_content_length": rag_result['original_content_length'],
                "processing_info": rag_result['processing_info'],
                "scraping_info": rag_result['scraping_info']
            }
            
        except ImportError as e:
            logger.error(f"‚ùå Erro ao importar sistema RAG existente: {e}")
            return {
                "success": False,
                "error": f"Sistema RAG n√£o dispon√≠vel: {e}",
                "agent_id": agent_id,
                "url": url,
                "rag_data": rag_result  # Retornar dados processados mesmo assim
            }
            
    except Exception as e:
        logger.error(f"‚ùå Erro na integra√ß√£o RAG: {e}")
        return {
            "success": False,
            "error": str(e),
            "agent_id": agent_id,
            "url": url
        }

def test_full_rag_integration():
    """Testa a integra√ß√£o completa com o sistema RAG existente."""
    print("\nüß™ Testando Integra√ß√£o Completa com Sistema RAG")
    print("=" * 60)
    
    # ID de agente de teste (usar um existente ou criar um novo)
    test_agent_id = "ae80adff-3ebd-4bc5-afbf-6e739df6d2fb"  # Agente existente do sistema
    test_url = "https://www.example.com"
    
    print(f"\n1. Testando integra√ß√£o completa:")
    print(f"   Agente: {test_agent_id}")
    print(f"   URL: {test_url}")
    
    result = integrate_with_existing_rag_system(
        url=test_url,
        agent_id=test_agent_id,
        chunk_size=600,
        chunk_overlap=100
    )
    
    if result['success']:
        print(f"‚úÖ Integra√ß√£o bem-sucedida!")
        print(f"üìÑ T√≠tulo: {result['title']}")
        print(f"üß© Chunks salvos: {result['chunks_saved']}")
        print(f"üîÆ Embeddings gerados: {result['embeddings_generated']}")
        print(f"üìè Conte√∫do original: {result['original_content_length']} caracteres")
        print(f"üè∑Ô∏è Agente: {result['agent_id']}")
        
        # Testar busca no sistema RAG
        try:
            from vector_store import PGVectorStore
            vector_store = PGVectorStore(agent_id=test_agent_id)
            
            # Fazer uma busca de teste
            search_results = vector_store.similarity_search("example domain", k=3)
            print(f"\nüîç Teste de busca:")
            print(f"   Query: 'example domain'")
            print(f"   Resultados encontrados: {len(search_results)}")
            
            if search_results:
                print(f"   Primeiro resultado: {search_results[0].page_content[:100]}...")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no teste de busca: {e}")
    else:
        print(f"‚ùå Falha na integra√ß√£o: {result['error']}")
        if 'rag_data' in result:
            print(f"üìä Dados RAG dispon√≠veis: {result['rag_data']['total_chunks']} chunks processados")

if __name__ == '__main__':
    # Verificar se deve testar OpenAI ou scraper normal
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == '--openai':
            test_openai_integration()
        elif sys.argv[1] == '--rag':
            test_full_rag_integration()
        elif sys.argv[1] == '--help':
            print("Uso:")
            print("  python scraper.py           # Teste b√°sico do scraper")
            print("  python scraper.py --openai  # Teste chunking + embeddings")
            print("  python scraper.py --rag     # Teste integra√ß√£o completa RAG")
        else:
            test_scraper()
    else:
        test_scraper() 