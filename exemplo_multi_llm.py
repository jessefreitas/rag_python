#!/usr/bin/env python3
"""
Exemplo pr√°tico de uso da funcionalidade Multi-LLM
Demonstra como comparar respostas de diferentes provedores de IA
"""

import requests
import json
import time
from typing import Dict, List, Any

class MultiLLMExample:
    """Exemplo de uso da funcionalidade Multi-LLM"""
    
    def __init__(self, base_url: str = "http://localhost:5000"):
        self.base_url = base_url
        self.api_base = f"{base_url}/api/v1"
        self.agent_id = None
        
    def setup_agent(self) -> bool:
        """Configura um agente para os testes"""
        print("üîß Configurando agente para testes...")
        
        # Criar um agente de teste
        agent_data = {
            "name": "Agente Multi-LLM Teste",
            "description": "Agente para testar compara√ß√£o de m√∫ltiplos LLMs",
            "system_prompt": "Voc√™ √© um assistente especializado em explicar conceitos t√©cnicos de forma clara e concisa.",
            "model_name": "gpt-3.5-turbo",
            "temperature": 0.7,
            "provider": "openai"
        }
        
        try:
            response = requests.post(f"{self.api_base}/agents", json=agent_data)
            if response.status_code == 201:
                result = response.json()
                self.agent_id = result['agent_id']
                print(f"‚úÖ Agente criado com ID: {self.agent_id}")
                return True
            else:
                print(f"‚ùå Erro ao criar agente: {response.text}")
                return False
        except Exception as e:
            print(f"‚ùå Erro na configura√ß√£o: {e}")
            return False
    
    def get_existing_agent(self) -> bool:
        """Obt√©m um agente existente"""
        try:
            response = requests.get(f"{self.api_base}/agents")
            agents = response.json()
            
            if agents:
                self.agent_id = agents[0]['id']
                print(f"‚úÖ Usando agente existente: {agents[0]['name']} (ID: {self.agent_id})")
                return True
            else:
                print("‚ùå Nenhum agente encontrado")
                return False
        except Exception as e:
            print(f"‚ùå Erro ao obter agentes: {e}")
            return False
    
    def compare_single_vs_multi(self, question: str):
        """Compara resposta √∫nica vs m√∫ltiplas respostas"""
        print(f"\nüîÑ Comparando: Single LLM vs Multi-LLM")
        print(f"üìù Pergunta: {question}")
        print("-" * 60)
        
        # 1. Resposta √∫nica (OpenAI)
        print("\n1Ô∏è‚É£ Resposta √önica (OpenAI):")
        try:
            response = requests.post(f"{self.api_base}/agents/{self.agent_id}/query", 
                                   json={"question": question, "use_multi_llm": False})
            result = response.json()
            
            if "error" not in result:
                print(f"   Modelo: {result['model_name']}")
                print(f"   Resposta: {result['response'][:200]}...")
                print(f"   Tamanho: {len(result['response'])} caracteres")
            else:
                print(f"   ‚ùå Erro: {result['error']}")
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")
        
        # 2. Respostas m√∫ltiplas
        print("\n2Ô∏è‚É£ Respostas M√∫ltiplas:")
        try:
            response = requests.post(f"{self.api_base}/agents/{self.agent_id}/query", 
                                   json={
                                       "question": question,
                                       "use_multi_llm": True,
                                       "providers": ["openai", "openrouter"]
                                   })
            result = response.json()
            
            if "error" not in result:
                print(f"   Provedores usados: {result['providers_used']}")
                
                for provider, response_data in result['responses'].items():
                    print(f"\n   üì° {provider.upper()}:")
                    print(f"      Modelo: {response_data['model']}")
                    print(f"      Resposta: {response_data['response'][:150]}...")
                    print(f"      Tamanho: {len(response_data['response'])} caracteres")
                
                # Estat√≠sticas de compara√ß√£o
                if result.get('comparison'):
                    comparison = result['comparison']
                    print(f"\n   üìä Estat√≠sticas:")
                    print(f"      Respostas √∫nicas: {comparison['unique_responses']}")
                    print(f"      Tamanhos: {comparison['response_lengths']}")
            else:
                print(f"   ‚ùå Erro: {result['error']}")
        except Exception as e:
            print(f"   ‚ùå Erro: {e}")
    
    def test_different_question_types(self):
        """Testa diferentes tipos de perguntas"""
        questions = [
            {
                "category": "Conceitos B√°sicos",
                "question": "O que √© intelig√™ncia artificial?"
            },
            {
                "category": "T√©cnico",
                "question": "Explique o que √© um algoritmo de machine learning."
            },
            {
                "category": "Comparativo",
                "question": "Qual a diferen√ßa entre supervised e unsupervised learning?"
            },
            {
                "category": "Aplica√ß√£o Pr√°tica",
                "question": "Como a IA pode ser aplicada na medicina?"
            },
            {
                "category": "Complexo",
                "question": "Explique o funcionamento de uma rede neural convolucional para processamento de imagens."
            }
        ]
        
        print(f"\nüß™ Testando Diferentes Tipos de Perguntas")
        print("=" * 60)
        
        for i, q_data in enumerate(questions, 1):
            print(f"\n{i}. {q_data['category']}: {q_data['question']}")
            
            try:
                response = requests.post(f"{self.api_base}/agents/{self.agent_id}/query", 
                                       json={
                                           "question": q_data['question'],
                                           "use_multi_llm": True,
                                           "providers": ["openai", "openrouter"]
                                       })
                result = response.json()
                
                if "error" not in result:
                    # Comparar tamanhos
                    lengths = result['comparison']['response_lengths']
                    longest = max(lengths, key=lengths.get)
                    shortest = min(lengths, key=lengths.get)
                    
                    print(f"   ‚úÖ Respostas obtidas de {len(result['responses'])} provedores")
                    print(f"   üìè Tamanhos: {lengths}")
                    print(f"   üìà Mais detalhada: {longest} ({lengths[longest]} chars)")
                    print(f"   üìâ Mais concisa: {shortest} ({lengths[shortest]} chars)")
                    
                    # Verificar se as respostas s√£o √∫nicas
                    unique_count = result['comparison']['unique_responses']
                    if unique_count > 1:
                        print(f"   üîÑ Respostas diferentes: {unique_count} vers√µes √∫nicas")
                    else:
                        print(f"   üîÑ Respostas similares: {unique_count} vers√£o √∫nica")
                else:
                    print(f"   ‚ùå Erro: {result['error']}")
                    
            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
            
            # Pausa entre perguntas
            time.sleep(2)
    
    def test_provider_specific_analysis(self):
        """An√°lise espec√≠fica por provedor"""
        print(f"\nüîç An√°lise Espec√≠fica por Provedor")
        print("=" * 60)
        
        question = "Explique o conceito de overfitting em machine learning."
        
        providers = ["openai", "openrouter"]
        
        for provider in providers:
            print(f"\nüì° Analisando {provider.upper()}:")
            
            try:
                response = requests.post(f"{self.api_base}/agents/{self.agent_id}/query", 
                                       json={
                                           "question": question,
                                           "use_multi_llm": True,
                                           "providers": [provider]
                                       })
                result = response.json()
                
                if "error" not in result and provider in result['responses']:
                    response_data = result['responses'][provider]
                    
                    # An√°lise da resposta
                    response_text = response_data['response']
                    word_count = len(response_text.split())
                    sentence_count = response_text.count('.') + response_text.count('!') + response_text.count('?')
                    
                    print(f"   Modelo: {response_data['model']}")
                    print(f"   Palavras: {word_count}")
                    print(f"   Frases: {sentence_count}")
                    print(f"   Caracteres: {len(response_text)}")
                    print(f"   M√©dia palavras/frase: {word_count/sentence_count:.1f}" if sentence_count > 0 else "   M√©dia palavras/frase: N/A")
                    
                    # Identificar caracter√≠sticas da resposta
                    if "overfitting" in response_text.lower():
                        print(f"   ‚úÖ Menciona o termo 'overfitting'")
                    if "training" in response_text.lower():
                        print(f"   ‚úÖ Menciona 'training'")
                    if "validation" in response_text.lower():
                        print(f"   ‚úÖ Menciona 'validation'")
                        
                else:
                    print(f"   ‚ùå Erro ou provedor n√£o dispon√≠vel")
                    
            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
    
    def test_feedback_system(self):
        """Testa o sistema de feedback"""
        print(f"\nüëç Testando Sistema de Feedback")
        print("=" * 60)
        
        question = "O que √© deep learning?"
        
        try:
            # Fazer consulta
            response = requests.post(f"{self.api_base}/agents/{self.agent_id}/query", 
                                   json={
                                       "question": question,
                                       "use_multi_llm": True,
                                       "providers": ["openai", "openrouter"]
                                   })
            result = response.json()
            
            if "error" not in result:
                print(f"üìù Pergunta: {question}")
                print(f"üì° Provedores: {result['providers_used']}")
                
                # Enviar feedback para cada resposta
                for provider, response_data in result['responses'].items():
                    print(f"\n   Avaliando {provider.upper()}:")
                    
                    # Simular avalia√ß√£o baseada no tamanho da resposta
                    response_length = len(response_data['response'])
                    if response_length > 200:
                        rating = "good"
                        reason = "resposta detalhada"
                    else:
                        rating = "bad"
                        reason = "resposta muito curta"
                    
                    # Enviar feedback
                    feedback_response = requests.post(f"{self.api_base}/agents/{self.agent_id}/feedback", 
                                                    json={
                                                        "user_input": question,
                                                        "agent_response": response_data['response'],
                                                        "rating": rating,
                                                        "provider": provider
                                                    })
                    
                    if feedback_response.json().get('success'):
                        print(f"   ‚úÖ Feedback {rating} enviado ({reason})")
                    else:
                        print(f"   ‚ùå Erro ao enviar feedback")
                        
            else:
                print(f"‚ùå Erro na consulta: {result['error']}")
                
        except Exception as e:
            print(f"‚ùå Erro no teste de feedback: {e}")
    
    def run_complete_demo(self):
        """Executa demonstra√ß√£o completa"""
        print("üöÄ Demonstra√ß√£o Completa - Sistema Multi-LLM")
        print("=" * 70)
        
        # Verificar se o servidor est√° rodando
        try:
            response = requests.get(f"{self.base_url}/")
            print("‚úÖ Servidor est√° rodando")
        except Exception as e:
            print(f"‚ùå Servidor n√£o est√° rodando: {e}")
            print("   Execute: python web_agent_manager.py")
            return
        
        # Configurar agente
        if not self.get_existing_agent():
            if not self.setup_agent():
                return
        
        # Executar testes
        self.compare_single_vs_multi("Explique o que √© machine learning em termos simples.")
        self.test_different_question_types()
        self.test_provider_specific_analysis()
        self.test_feedback_system()
        
        print(f"\n" + "=" * 70)
        print("üéâ Demonstra√ß√£o conclu√≠da!")
        print("\nüí° Pr√≥ximos passos:")
        print("   - Acesse http://localhost:5000 para usar a interface web")
        print("   - Teste diferentes combina√ß√µes de provedores")
        print("   - Monitore as estat√≠sticas no dashboard")
        print("   - Use o sistema de feedback para melhorar as respostas")

def main():
    """Fun√ß√£o principal"""
    demo = MultiLLMExample()
    demo.run_complete_demo()

if __name__ == "__main__":
    main() 