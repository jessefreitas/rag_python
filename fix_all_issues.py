#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”§ RAG Python - Corretor Universal de Problemas
==================================================
Script para corrigir todos os problemas e rodar o sistema completo
"""

import os
import sys
import time
import json
import psutil
import subprocess
import importlib.util
from pathlib import Path

def print_header():
    """Exibe cabeÃ§alho do script"""
    print("=" * 70)
    print("ğŸ”§ RAG PYTHON v1.5.1 - CORRETOR UNIVERSAL")
    print("=" * 70)
    print("ğŸ¯ Corrigindo todos os problemas e iniciando sistema completo")
    print("=" * 70)

def kill_processes_on_port(port):
    """Mata processos rodando em uma porta especÃ­fica"""
    try:
        killed_count = 0
        for proc in psutil.process_iter(['pid', 'name', 'connections']):
            try:
                connections = proc.info['connections']
                if connections:
                    for conn in connections:
                        if hasattr(conn, 'laddr') and conn.laddr and conn.laddr.port == port:
                            print(f"  ğŸ”« Matando processo {proc.info['name']} (PID: {proc.info['pid']}) na porta {port}")
                            proc.kill()
                            killed_count += 1
                            break
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                continue
        
        if killed_count > 0:
            print(f"  âœ… {killed_count} processo(s) encerrado(s) na porta {port}")
        else:
            print(f"  âœ… Porta {port} livre")
        return True
    except Exception as e:
        print(f"  âš ï¸ Erro ao verificar porta {port}: {e}")
        return False

def check_and_fix_llm_models_config():
    """Verifica e corrige o arquivo llm_models_config.py"""
    print("\nğŸ”§ VERIFICANDO llm_models_config.py...")
    
    try:
        # Tentar importar
        import llm_models_config
        importlib.reload(llm_models_config)
        
        # Verificar se tem o mÃ©todo necessÃ¡rio
        if hasattr(llm_models_config.models_manager, 'get_provider_models_simple'):
            print("  âœ… llm_models_config.py OK - mÃ©todo get_provider_models_simple encontrado")
            return True
        else:
            print("  âŒ MÃ©todo get_provider_models_simple nÃ£o encontrado")
            return False
            
    except SyntaxError as e:
        print(f"  âŒ Erro de sintaxe em llm_models_config.py: {e}")
        print("  ğŸ”„ Recriando arquivo...")
        
        # Recriar arquivo limpo
        recreate_llm_models_config()
        return True
        
    except Exception as e:
        print(f"  âŒ Erro ao importar llm_models_config.py: {e}")
        return False

def recreate_llm_models_config():
    """Recria o arquivo llm_models_config.py completamente limpo"""
    content = '''# -*- coding: utf-8 -*-
"""
ğŸ¤– RAG Python - ConfiguraÃ§Ã£o de Modelos LLM v1.5.3
==================================================
CatÃ¡logo completo de modelos de diferentes provedores
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Any

@dataclass
class ModelInfo:
    """InformaÃ§Ãµes de um modelo LLM"""
    name: str
    provider: str
    description: str
    context_length: int
    supports_vision: bool = False
    supports_function_calling: bool = False
    cost_per_1k_tokens: float = 0.0
    emoji: str = "ğŸ¤–"

class ModelsManager:
    """Gerenciador central de modelos LLM"""
    
    def __init__(self):
        self.models = self._initialize_models()
    
    def _initialize_models(self) -> Dict[str, ModelInfo]:
        """Inicializa catÃ¡logo de modelos"""
        return {
            # OpenAI Models
            "gpt-4o": ModelInfo(
                name="gpt-4o",
                provider="openai",
                description="ğŸš€ GPT-4o - Modelo mais avanÃ§ado da OpenAI",
                context_length=128000,
                supports_vision=True,
                supports_function_calling=True,
                cost_per_1k_tokens=0.005,
                emoji="ğŸš€"
            ),
            "gpt-4o-mini": ModelInfo(
                name="gpt-4o-mini",
                provider="openai",
                description="âš¡ GPT-4o Mini - RÃ¡pido e eficiente",
                context_length=128000,
                supports_vision=True,
                supports_function_calling=True,
                cost_per_1k_tokens=0.00015,
                emoji="âš¡"
            ),
            "gpt-4-turbo": ModelInfo(
                name="gpt-4-turbo",
                provider="openai",
                description="ğŸï¸ GPT-4 Turbo - Alto desempenho",
                context_length=128000,
                supports_vision=True,
                supports_function_calling=True,
                cost_per_1k_tokens=0.01,
                emoji="ğŸï¸"
            ),
            "gpt-3.5-turbo": ModelInfo(
                name="gpt-3.5-turbo",
                provider="openai",
                description="ğŸ’¨ GPT-3.5 Turbo - RÃ¡pido e confiÃ¡vel",
                context_length=16385,
                supports_function_calling=True,
                cost_per_1k_tokens=0.0005,
                emoji="ğŸ’¨"
            ),
            
            # Google Models
            "gemini-1.5-pro": ModelInfo(
                name="gemini-1.5-pro",
                provider="google",
                description="ğŸ’ Gemini 1.5 Pro - Modelo premium do Google",
                context_length=2000000,
                supports_vision=True,
                supports_function_calling=True,
                cost_per_1k_tokens=0.0035,
                emoji="ğŸ’"
            ),
            "gemini-1.5-flash": ModelInfo(
                name="gemini-1.5-flash",
                provider="google",
                description="âš¡ Gemini 1.5 Flash - UltrarrÃ¡pido",
                context_length=1000000,
                supports_vision=True,
                supports_function_calling=True,
                cost_per_1k_tokens=0.00015,
                emoji="âš¡"
            ),
            "gemini-pro": ModelInfo(
                name="gemini-pro",
                provider="google",
                description="ğŸŒŸ Gemini Pro - Modelo principal do Google",
                context_length=32768,
                supports_function_calling=True,
                cost_per_1k_tokens=0.0005,
                emoji="ğŸŒŸ"
            ),
            
            # OpenRouter Models
            "anthropic/claude-3.5-sonnet": ModelInfo(
                name="anthropic/claude-3.5-sonnet",
                provider="openrouter",
                description="ğŸ¼ Claude 3.5 Sonnet - Excelente para escrita",
                context_length=200000,
                supports_function_calling=True,
                cost_per_1k_tokens=0.003,
                emoji="ğŸ¼"
            ),
            "openai/gpt-4o": ModelInfo(
                name="openai/gpt-4o",
                provider="openrouter",
                description="ğŸš€ GPT-4o via OpenRouter",
                context_length=128000,
                supports_vision=True,
                supports_function_calling=True,
                cost_per_1k_tokens=0.005,
                emoji="ğŸš€"
            ),
            
            # DeepSeek Models
            "deepseek-chat": ModelInfo(
                name="deepseek-chat",
                provider="deepseek",
                description="ğŸ”® DeepSeek Chat - ConversaÃ§Ã£o avanÃ§ada",
                context_length=32768,
                supports_function_calling=True,
                cost_per_1k_tokens=0.0001,
                emoji="ğŸ”®"
            ),
            "deepseek-coder": ModelInfo(
                name="deepseek-coder",
                provider="deepseek",
                description="ğŸ’» DeepSeek Coder - Especialista em programaÃ§Ã£o",
                context_length=16384,
                supports_function_calling=True,
                cost_per_1k_tokens=0.0001,
                emoji="ğŸ’»"
            )
        }
    
    def get_models_by_provider(self, provider: str) -> List[ModelInfo]:
        """Retorna modelos de um provedor especÃ­fico"""
        return [model for model in self.models.values() if model.provider == provider]
    
    def get_model_info(self, model_name: str) -> ModelInfo:
        """Retorna informaÃ§Ãµes de um modelo especÃ­fico"""
        return self.models.get(model_name)
    
    def get_available_providers(self) -> List[str]:
        """Retorna lista de provedores disponÃ­veis"""
        providers = set(model.provider for model in self.models.values())
        return sorted(list(providers))
    
    def get_provider_models_simple(self) -> Dict[str, List[str]]:
        """Retorna dicionÃ¡rio simples de modelos por provedor"""
        result = {}
        for provider in self.get_available_providers():
            result[provider] = [model.name for model in self.get_models_by_provider(provider)]
        return result
    
    def get_provider_stats(self) -> Dict[str, Dict[str, Any]]:
        """Retorna estatÃ­sticas por provedor"""
        stats = {}
        for provider in self.get_available_providers():
            provider_models = self.get_models_by_provider(provider)
            stats[provider] = {
                'total_models': len(provider_models),
                'models': [m.name for m in provider_models]
            }
        return stats

# InstÃ¢ncia global do gerenciador
models_manager = ModelsManager()

# Mapeamento de compatibilidade
PROVIDER_MODEL_MAP = {
    'openai': ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-3.5-turbo'],
    'google': ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-pro'],
    'openrouter': ['anthropic/claude-3.5-sonnet', 'openai/gpt-4o'],
    'deepseek': ['deepseek-chat', 'deepseek-coder']
}

if __name__ == "__main__":
    print("ğŸ¤– RAG Python - CatÃ¡logo de Modelos LLM v1.5.3")
    print("=" * 50)
    
    stats = models_manager.get_provider_stats()
    for provider, data in stats.items():
        print(f"ğŸ“Š {provider.upper()}: {data['total_models']} modelos")
    
    print(f"âœ… Total: {len(models_manager.models)} modelos carregados")
'''
    
    with open('llm_models_config.py', 'w', encoding='utf-8') as f:
        f.write(content)
    
    print("  âœ… Arquivo llm_models_config.py recriado com sucesso")

def check_environment():
    """Verifica variÃ¡veis de ambiente"""
    print("\nğŸ”§ VERIFICANDO VARIÃVEIS DE AMBIENTE...")
    
    env_vars = {
        'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
        'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY'),
        'OPENROUTER_API_KEY': os.getenv('OPENROUTER_API_KEY'),
        'DEEPSEEK_API_KEY': os.getenv('DEEPSEEK_API_KEY')
    }
    
    configured = 0
    for var, value in env_vars.items():
        if value:
            print(f"  âœ… {var} configurada")
            configured += 1
        else:
            print(f"  âŒ {var} nÃ£o configurada")
    
    print(f"  ğŸ“Š {configured}/4 provedores configurados")
    return configured > 0

def start_server(script_name, port, name, background=True):
    """Inicia um servidor em background"""
    print(f"\nğŸš€ INICIANDO {name}...")
    
    # Matar processos existentes na porta
    kill_processes_on_port(port)
    time.sleep(2)
    
    try:
        if background:
            # Iniciar em background
            if sys.platform == "win32":
                process = subprocess.Popen(
                    ['python', script_name],
                    creationflags=subprocess.CREATE_NEW_CONSOLE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
            else:
                process = subprocess.Popen(
                    ['python', script_name],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
            
            time.sleep(3)  # Dar tempo para inicializar
            
            # Verificar se ainda estÃ¡ rodando
            if process.poll() is None:
                print(f"  âœ… {name} iniciado com sucesso na porta {port}")
                print(f"  ğŸ“¡ URL: http://localhost:{port}")
                return True
            else:
                stdout, stderr = process.communicate()
                print(f"  âŒ Erro ao iniciar {name}:")
                if stderr:
                    print(f"      {stderr.decode()[:200]}...")
                return False
        else:
            print(f"  ğŸ“¡ Executando: python {script_name}")
            print(f"  ğŸŒ URL: http://localhost:{port}")
            print(f"  â¹ï¸ Pressione Ctrl+C para parar")
            subprocess.run(['python', script_name])
            return True
            
    except Exception as e:
        print(f"  âŒ Erro ao iniciar {name}: {e}")
        return False

def main():
    """FunÃ§Ã£o principal"""
    print_header()
    
    # 1. Verificar e corrigir llm_models_config.py
    if not check_and_fix_llm_models_config():
        print("âŒ NÃ£o foi possÃ­vel corrigir llm_models_config.py")
        return False
    
    # 2. Verificar ambiente
    if not check_environment():
        print("âš ï¸ Nenhuma API key configurada, sistema funcionarÃ¡ em modo limitado")
    
    # 3. Limpar portas
    print("\nğŸ§¹ LIMPANDO PORTAS...")
    ports_to_clean = [8501, 5000, 5001, 5002]
    for port in ports_to_clean:
        kill_processes_on_port(port)
    
    print("\n" + "=" * 70)
    print("ğŸ¯ ESCOLHA O MODO DE INICIALIZAÃ‡ÃƒO:")
    print("=" * 70)
    print("1. ğŸš€ COMPLETO - Todos os servidores em background")
    print("2. ğŸ¯ STREAMLIT - Apenas interface principal (porta 8501)")
    print("3. ğŸ”Œ API FLASK - Apenas API para extensÃ£o (porta 5000)")
    print("4. ğŸ—„ï¸ API SUPABASE - API com Supabase (porta 5002)")
    print("5. âš™ï¸ MANUAL - Escolher individualmente")
    print("=" * 70)
    
    try:
        choice = input("Digite sua escolha (1-5): ").strip()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ OperaÃ§Ã£o cancelada pelo usuÃ¡rio")
        return
    
    if choice == "1":
        print("\nğŸš€ MODO COMPLETO - Iniciando todos os servidores...")
        
        # Iniciar todos em background
        servers = [
            ('api_server_simple.py', 5000, 'API Flask Simples'),
            ('api_server_supabase.py', 5002, 'API Supabase'),
        ]
        
        success_count = 0
        for script, port, name in servers:
            if start_server(script, port, name, background=True):
                success_count += 1
        
        print(f"\nğŸ“Š RESULTADO: {success_count}/{len(servers)} servidores iniciados")
        
        # Streamlit por Ãºltimo (foreground)
        print("\nğŸ¯ Iniciando Streamlit (interface principal)...")
        start_server('iniciar_servidor_rag.py', 8501, 'Streamlit RAG', background=False)
        
    elif choice == "2":
        print("\nğŸ¯ MODO STREAMLIT - Apenas interface principal...")
        start_server('iniciar_servidor_rag.py', 8501, 'Streamlit RAG', background=False)
        
    elif choice == "3":
        print("\nğŸ”Œ MODO API FLASK - Apenas API para extensÃ£o...")
        start_server('api_server_simple.py', 5000, 'API Flask', background=False)
        
    elif choice == "4":
        print("\nğŸ—„ï¸ MODO API SUPABASE - API com Supabase...")
        start_server('api_server_supabase.py', 5002, 'API Supabase', background=False)
        
    elif choice == "5":
        print("\nâš™ï¸ MODO MANUAL - Escolha individual...")
        
        servers = [
            ('iniciar_servidor_rag.py', 8501, 'Streamlit RAG (Interface Principal)'),
            ('api_server_simple.py', 5000, 'API Flask Simples'),
            ('api_server_supabase.py', 5002, 'API Supabase'),
        ]
        
        for i, (script, port, name) in enumerate(servers, 1):
            resp = input(f"Iniciar {name}? (s/n): ").strip().lower()
            if resp in ['s', 'sim', 'y', 'yes']:
                bg = input("Em background? (s/n): ").strip().lower() not in ['s', 'sim', 'y', 'yes']
                start_server(script, port, name, background=not bg)
                if not bg:  # Se nÃ£o Ã© background, para aqui
                    break
    else:
        print("âŒ OpÃ§Ã£o invÃ¡lida")
        return
    
    print("\n" + "=" * 70)
    print("âœ… SISTEMA RAG PYTHON INICIADO COM SUCESSO!")
    print("=" * 70)
    print("ğŸŒ URLs disponÃ­veis:")
    print("  - Streamlit: http://localhost:8501")
    print("  - API Flask: http://localhost:5000")
    print("  - API Supabase: http://localhost:5002")
    print("=" * 70)

if __name__ == "__main__":
    main() 